# -*- coding: utf-8 -*-
"""ablation_testing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TvOe-RDgviC9XjvJ_kh5dg5jcMfxe-c

### Notebook Objective: Calculate the Feature Maps from Different CNN Models

### Combinations which need to be worked through:

1.                                     FINETUNE, NO FINETUNE

UNTRAINED SIMPLE CNN MODEL,
CNN PRETRAINED ON TEXT,
CNN PRETRAINED ON IMAGENET,
TRAINED ON SIMPLE WORLD OF DAVIDA,
"""

!pip install torchvision

### LIBRARY IMPORTS
from google.colab import files
from torchvision import models
from PIL import Image
import os

from google.colab import drive
drive.mount('/content/drive')

resnet = models.resnet18(pretrained=True)

"""### Texting some pretrained images"""

arrow_down = Image.open("/content/drive/MyDrive/iscr_image_dataset/arrow_down.png").convert('RGB')
arrow_up = Image.open("/content/drive/MyDrive/iscr_image_dataset/arrow_up.png").convert('RGB')

from torchvision import transforms
import torch
#
# Create a preprocessing pipeline
#
preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )])
#
# Pass the image for preprocessing and the image preprocessed
#
down_img_preprocessed = preprocess(arrow_down)
up_img_preprocessed = preprocess(arrow_up)

#
# Reshape, crop, and normalize the input tensor for feeding into network for evaluation
#
up_img_tensor = torch.unsqueeze(up_img_preprocessed, 0)
down_img_tensor = torch.unsqueeze(down_img_preprocessed, 0)

resnet.eval()
#
# Get the predictions of image as scores related to how the loaded image
# matches with 1000 ImageNet classes. The variable, out is a vector of 1000 scores
#
out_up = resnet(up_img_tensor)
out_down = resnet(down_img_tensor)

import matplotlib.pyplot as plt

def hook(module, input, output):
    x = output.detach().clone()
    x = x.squeeze()
    # we only visualize the first 10 features
    for i in range(min(x.shape[0], 10)):
        plt.subplot(2, 5, i+1)
        plt.imshow(x[i])
    # plt.show()

# Choose the layer to visualize
target_layer = resnet.layer1[1].conv2
handle = target_layer.register_forward_hook(hook)

# Run the model on our transformed image
resnet(up_img_tensor)

# Don't forget to remove the hook when you're done
handle.remove()

def hook2(module, input, output):
    x = output.detach().clone()
    x = x.squeeze()
    for i in range(min(x.shape[0], 10)):
        plt.subplot(2, 5, i+1)
        plt.imshow(x[i])

layers = [resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4]

hooks = []
for layer in layers:
    for block in layer:
        # You can attach to `conv1`, `conv2`, etc. within each block
        h = block.conv1.register_forward_hook(hook2)
        hooks.append(h)

output = resnet(up_img_tensor)

for h in hooks:
    h.remove()

target_layer = resnet.layer1[0].conv1
handle = target_layer.register_forward_hook(hook)

# Run the model on our transformed image
resnet(down_img_tensor)

# Don't forget to remove the hook when you're done
handle.remove()

"""# Ablation Studies: Goal: ResNet Network Behavior for Our Image Dataset

### Ablation Studies


Ablation testing in the context of Convolutional Neural Networks (CNNs) refers to a process of systematically removing or altering specific components of the network to understand their impact on the network's performance, particularly when dealing with rotating images. This technique helps researchers and practitioners gain insights into which parts of the network are crucial for handling certain variations in the input data, such as rotation in this case.

Here's how ablation testing could be applied to CNNs for rotating images:


```
# This is formatted as code
```


1. **Problem Statement**: Let's say you're working on a task that involves classifying rotated images using a CNN. You want to determine which layers or components of the network contribute the most to accurate classification despite rotations.

2. **Baseline CNN**: First, you would train a baseline CNN on your dataset without any modifications. This serves as your reference model.

3. **Ablation Testing**:
   - **Layer Ablation**: You start by systematically disabling or removing certain layers from the network. For example, you might remove specific convolutional layers or pooling layers.
   - **Feature Ablation**: Another approach is to ablate specific features or channels within a layer. For instance, you could set certain feature maps to zero or remove specific channels.
   - **Rotation-Invariant Layer Ablation**: To test the network's performance under rotations, you might focus on layers that are particularly important for learning rotation-invariant features, such as specific convolutional or pooling layers.

4. **Evaluation**: After each ablation, you evaluate the modified network's performance on the task of classifying rotated images. You record metrics like accuracy, loss, and perhaps the amount of rotation the network can tolerate before performance degrades significantly.

5. **Insights**: By comparing the performance of the baseline CNN with the ablated versions, you can gain insights into the importance of different layers or components for handling rotations. This helps you understand how the network's architecture contributes to rotation invariance.

6. **Interpretation**: Depending on the results, you can draw conclusions about which layers are more responsible for handling rotations. This might guide future network design or modifications to improve rotation-invariant classification.

Ablation testing is a useful technique to gain a deeper understanding of neural network behavior and to identify which elements are crucial for handling specific variations in the data. It's important to note that ablation testing can be time-consuming and may require substantial computational resources, especially for complex networks. Additionally, while ablation can reveal the importance of certain components, it might not provide a full understanding of the network's internal representations and dynamics.

If your goal is to understand how a CNN calculates the pose of an image by examining the changes in feature maps for various orientations of the same image, here's a step-by-step plan:

1. **Choose a Pre-trained CNN or Train Your Own**: Depending on your resources and the specific problem, you might start with a pre-trained model (e.g., VGG16, ResNet, or MobileNet) or train your own CNN. If you're training your own, ensure you have a dataset with labeled pose information.

2. **Prepare Rotated Images**:
   - Choose a specific image for testing.
   - Create variations of this image by systematically rotating it through different angles (e.g., 0째, 45째, 90째, 135째, etc.).
   - Make sure rotations are applied without introducing additional artifacts or distortions.

3. **Feed the Rotated Images to the CNN**: For each rotated image, pass it through the CNN. This will give you feature maps at each layer of the network.

4. **Visualize Feature Maps**:
   - For each rotation, extract and visualize the feature maps at various layers. Depending on the depth of the CNN, you might choose initial, middle, and deeper layers.
   - Use heatmaps or grayscale representations to visualize the activations. Look for changes in the patterns as you compare the feature maps for each rotated image.

5. **Comparison and Analysis**:
   - Compare the feature maps for each orientation. Take note of which layers seem to be most sensitive to rotation changes and which layers are more invariant.
   - Look for patterns or specific features that activate consistently, regardless of the image's rotation.
   - Compare deeper layers to see if and how rotational information is preserved or abstracted as you progress through the network.

6. **Draw Conclusions**:
   - Determine which layers are most indicative of capturing pose-related information.
   - Understand how the spatial hierarchy of the CNN processes rotational changes. Often, earlier layers capture basic edges and textures, while deeper layers might capture more complex patterns and shapes that could be indicative of pose.

7. **Further Exploration** (optional):
   - To better understand pose calculation, consider incorporating specialized architectures or layers designed for rotation invariance, such as rotation-equivariant CNNs or spatial transformer networks.
   - Compare the behavior of standard CNNs with these specialized networks in terms of their feature map responses to rotated images.

Remember that understanding feature maps visually can be quite challenging, especially in deeper layers, due to the high-dimensionality and abstraction. However, with careful analysis and perhaps supplementary methods like dimensionality reduction or activation maximization, you can gain insights into how a CNN processes and understands image orientation.
"""

resnet_18 = models.resnet18(pretrained=True)
resnet_18.eval()

print(resnet_18)

"""## We add Hooks for the Display of Feature Maps"""

import torch
import torchvision.models as models
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image

# Load the pre-trained ResNet-18 model
resnet_18 = models.resnet18(pretrained=True)
resnet_18.eval()

# Define the preprocessing transformations (assuming ImageNet statistics)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Collect feature maps
def collect_feature_maps(model, image, layers):
    feature_maps = {}
    hooks = []

    def hook_fn(module, input, output, key):
        feature_maps[key] = output

    for key, layer in layers.items():
        hooks.append(layer.register_forward_hook(lambda module, input, output, key=key: hook_fn(module, input, output, key)))

    with torch.no_grad():
        output = model(image)

    for hook in hooks:
        hook.remove()

    return feature_maps

# Visualize feature maps
def plot_feature_maps(key, feature_map):
    fmap = feature_map.cpu().squeeze().numpy()
    num_features = fmap.shape[0]
    grid_size = min(int(num_features**0.5), num_features)

    fig, axarr = plt.subplots(grid_size, grid_size, figsize=(20, 20))
    for idx in range(grid_size * grid_size):
        row = idx // grid_size
        col = idx % grid_size
        if idx < num_features:
          if fmap[idx].size != 1:  # ensures the tensor isn't scalar
            axarr[row, col].imshow(fmap[idx], cmap="inferno")
          else:
              print(f"Skipping scalar feature map at index {idx}.")
          axarr[row, col].axis('off')
        else:
            axarr[row, col].set_visible(False)
    plt.suptitle(f'Feature Maps from {key} layer', fontsize=16)
    plt.show()

# Define layers of interest
layers = {
    "beginning": resnet_18.conv1,
    "middle": resnet_18.layer2[-1],
    "end": list(resnet_18.children())[-2]
}

folder_path = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"


for filename in os.listdir(folder_path):
    if (filename.endswith(".jpg") or filename.endswith(".png")) and filename.startswith("shape_000"):
      count += 1
      file_path = os.path.join(folder_path, filename)
      img = Image.open(file_path).convert('RGB')
      img_preprocessed = preprocess(img)
      img_tensor = torch.unsqueeze(img_preprocessed, 0)
      fmaps = collect_feature_maps(resnet_18, img_tensor, layers)
      for key in ["beginning", "middle", "end"]:
          plot_feature_maps(key, fmaps[key])

"""# ROTATED IMAGES PERCEPTION DIFFERENCES

Cover all the datasets:

1. Shapes
2. Numbers

1. Put the images in a class
2. See the different ways a CNN works on it
3. Understand the metric change in a rotated image of the sameclass
"""

# MEAN ACTIVATION MAPS:

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image



image_folder = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"


# Define custom dataset for loading images
class CustomDataset(Dataset):
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img

# Load the ResNet-18 model
model = models.resnet18(pretrained=True)
model.eval()

# Disable gradient computation
with torch.no_grad():
    # Choose the desired layer for activations
    # Let's choose the layer named "layer3" as an example
    layer_name = 'layer3'
    layer = dict(model.named_children())[layer_name]

    # Define a forward hook to capture the activations
    activations = []
    def hook_fn(module, input, output):
        activations.append(output)

    hook = layer.register_forward_hook(hook_fn)

    # Image transformation pipeline
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # List of image paths
    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.startswith('shape_000')] # Adjust as per your naming scheme # and so on...
    dataset = CustomDataset(image_paths=image_paths, transform=transform)
    dataloader = DataLoader(dataset, batch_size=len(image_paths), shuffle=False)

    # Pass images through the model
    for imgs in dataloader:
        outputs = model(imgs)

    hook.remove()

    # Compute mean activation maps
    mean_activation = torch.mean(torch.stack(activations), dim=0)

    print(mean_activation.shape) # This will give you the shape of the mean activation map

"""## METRICS BASED

 1. Mean Activation Values
 2. Histogram Maps
 3. Euclidean Distance
 4. Gram Comparison
 5. PCA
 6. LIME
 7. SHAP
 8. GRAD-CAM
"""

image_folder = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"
image_path = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.startswith('shape_000')]

len(image_path)



import torchvision.models as models
import torchvision.transforms as transforms

# Load ResNet-18
model = models.resnet18(pretrained=True).eval()

# Define the hook and attach to a layer
activations = []
def hook_fn(module, input, output):
    activations.append(output)

layer = model.layer2[1]
h = layer.register_forward_hook(hook_fn)

# Define transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Pass each rotated image through ResNet-18
for img in image_path:
  img = Image.open(image_path).convert('RGB')
  input_img = transform(img).unsqueeze(0)
  output = model(input_img)

# Visualize the captured activations for each rotated image
for angle, activation in zip(angles, activations):
    mean_activation = torch.mean(activation, dim=0).squeeze()
    print(f"Angle: {angle} degrees")
    print(mean_activation)

# Remove the hook
h.remove()





"""## LIME: Local Interpretable Model Agnostic Explanations

"""



!pip install lime

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

model = models.resnet18(pretrained=True)
model.eval()

def preprocess_image(pil_image):
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(pil_image).unsqueeze(0)

def get_image_predictions(numpy_img):
    # Convert numpy array back to PIL Image
    # if len(numpy_img.shape) == 4:
    #   numpy_img = numpy_img.squeeze(0)
    numpy_img = numpy_img[0]
    pil_img = Image.fromarray((numpy_img * 255).astype(np.uint8))
    tensor_img = preprocess_image(pil_img)

    with torch.no_grad():
        outputs = model(tensor_img)
    return torch.nn.functional.softmax(outputs, dim=1).numpy()

from PIL import Image
from IPython.display import display

# LOADING IMAGES

# Load image
image_folder = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"
image_path = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.startswith('shape_000')]


# GREYSCALE, SINCE DAVIDA'S IMAGES WERE GRAY

pil_image = Image.open(image_path[0])
print(pil_image)

width, height = pil_image.size
# Print the dimensions
print(f"Width: {width}, Height: {height}")



numpy_image = np.array(pil_image)
numpy_image_rgb = numpy_image[:, :, :3]
numpy_image_rgb.shape

# Initialize LIME explainer
explainer = lime_image.LimeImageExplainer()

# Explain the image



explanation = explainer.explain_instance(numpy_image_rgb,
                                         get_image_predictions,
                                         top_labels=5,
                                         hide_color=0,
                                         num_samples=1000)

# Get the top label (you can change this to any label of interest)
top_label = explanation.top_labels[0]
temp, mask = explanation.get_image_and_mask(top_label, positive_only=True, num_features=5, hide_rest=True)

# Display the image and explanation
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.show()

"""## SHAP: (SHapley Additive exPlanations)




"""

!pip install shap

import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50
from PIL import Image
import shap
import numpy as np

model = models.resnet18(pretrained=True)
model.eval()

image_folder = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"
image_path = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.startswith('shape_000')]


# GREYSCALE, SINCE DAVIDA'S IMAGES WERE GRAY

def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img)
    return img_tensor.unsqueeze(0)

background = torch.randn((5, 3, 224, 224))

input_img = preprocess_image(image_path[0])

explainer = shap.DeepExplainer(model, background)
shap_values = explainer.shap_values(input_img)

shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]
img_numpy = np.swapaxes(np.swapaxes(input_img[0].numpy(), 0, -1), 0, 1)
shap.image_plot(shap_numpy, -img_numpy)

"""## Ablation Study in Paper: Ablation Studies in Artificial Neural Networks

# All ResNet Model Comparison
"""

