# -*- coding: utf-8 -*-
"""ablation_testing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TvOe-RDgviC9XjvJ_kh5dg5jcMfxe-c

### Notebook Objective: Calculate the Feature Maps from Different CNN Models

### Combinations which need to be worked through:

1.                                     FINETUNE, NO FINETUNE

UNTRAINED SIMPLE CNN MODEL,
CNN PRETRAINED ON TEXT,
CNN PRETRAINED ON IMAGENET,
TRAINED ON SIMPLE WORLD OF DAVIDA,
"""

!pip install torchvision

### LIBRARY IMPORTS
from google.colab import files
from torchvision import models
from PIL import Image
import os

from google.colab import drive
drive.mount('/content/drive')

resnet = models.resnet18(pretrained=True)

"""### Texting some pretrained images"""

arrow_down = Image.open("/content/drive/MyDrive/iscr_image_dataset/arrow_down.png").convert('RGB')
arrow_up = Image.open("/content/drive/MyDrive/iscr_image_dataset/arrow_up.png").convert('RGB')

from torchvision import transforms
import torch
#
# Create a preprocessing pipeline
#
preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )])
#
# Pass the image for preprocessing and the image preprocessed
#
down_img_preprocessed = preprocess(arrow_down)
up_img_preprocessed = preprocess(arrow_up)

#
# Reshape, crop, and normalize the input tensor for feeding into network for evaluation
#
up_img_tensor = torch.unsqueeze(up_img_preprocessed, 0)
down_img_tensor = torch.unsqueeze(down_img_preprocessed, 0)

resnet.eval()
#
# Get the predictions of image as scores related to how the loaded image
# matches with 1000 ImageNet classes. The variable, out is a vector of 1000 scores
#
out_up = resnet(up_img_tensor)
out_down = resnet(down_img_tensor)

import matplotlib.pyplot as plt

def hook(module, input, output):
    x = output.detach().clone()
    x = x.squeeze()
    # we only visualize the first 10 features
    for i in range(min(x.shape[0], 10)):
        plt.subplot(2, 5, i+1)
        plt.imshow(x[i])
    # plt.show()

# Choose the layer to visualize
target_layer = resnet.layer1[1].conv2
handle = target_layer.register_forward_hook(hook)

# Run the model on our transformed image
resnet(up_img_tensor)

# Don't forget to remove the hook when you're done
handle.remove()

def hook2(module, input, output):
    x = output.detach().clone()
    x = x.squeeze()
    for i in range(min(x.shape[0], 10)):
        plt.subplot(2, 5, i+1)
        plt.imshow(x[i])

layers = [resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4]

hooks = []
for layer in layers:
    for block in layer:
        # You can attach to `conv1`, `conv2`, etc. within each block
        h = block.conv1.register_forward_hook(hook2)
        hooks.append(h)

output = resnet(up_img_tensor)

for h in hooks:
    h.remove()

target_layer = resnet.layer1[0].conv1
handle = target_layer.register_forward_hook(hook)

# Run the model on our transformed image
resnet(down_img_tensor)

# Don't forget to remove the hook when you're done
handle.remove()

"""### Ablation Studies:

Ablation testing in the context of Convolutional Neural Networks (CNNs) refers to a process of systematically removing or altering specific components of the network to understand their impact on the network's performance, particularly when dealing with rotating images. This technique helps researchers and practitioners gain insights into which parts of the network are crucial for handling certain variations in the input data, such as rotation in this case.

Here's how ablation testing could be applied to CNNs for rotating images:

1. **Problem Statement**: Let's say you're working on a task that involves classifying rotated images using a CNN. You want to determine which layers or components of the network contribute the most to accurate classification despite rotations.

2. **Baseline CNN**: First, you would train a baseline CNN on your dataset without any modifications. This serves as your reference model.

3. **Ablation Testing**:
   - **Layer Ablation**: You start by systematically disabling or removing certain layers from the network. For example, you might remove specific convolutional layers or pooling layers.
   - **Feature Ablation**: Another approach is to ablate specific features or channels within a layer. For instance, you could set certain feature maps to zero or remove specific channels.
   - **Rotation-Invariant Layer Ablation**: To test the network's performance under rotations, you might focus on layers that are particularly important for learning rotation-invariant features, such as specific convolutional or pooling layers.

4. **Evaluation**: After each ablation, you evaluate the modified network's performance on the task of classifying rotated images. You record metrics like accuracy, loss, and perhaps the amount of rotation the network can tolerate before performance degrades significantly.

5. **Insights**: By comparing the performance of the baseline CNN with the ablated versions, you can gain insights into the importance of different layers or components for handling rotations. This helps you understand how the network's architecture contributes to rotation invariance.

6. **Interpretation**: Depending on the results, you can draw conclusions about which layers are more responsible for handling rotations. This might guide future network design or modifications to improve rotation-invariant classification.

Ablation testing is a useful technique to gain a deeper understanding of neural network behavior and to identify which elements are crucial for handling specific variations in the data. It's important to note that ablation testing can be time-consuming and may require substantial computational resources, especially for complex networks. Additionally, while ablation can reveal the importance of certain components, it might not provide a full understanding of the network's internal representations and dynamics.

If your goal is to understand how a CNN calculates the pose of an image by examining the changes in feature maps for various orientations of the same image, here's a step-by-step plan:

1. **Choose a Pre-trained CNN or Train Your Own**: Depending on your resources and the specific problem, you might start with a pre-trained model (e.g., VGG16, ResNet, or MobileNet) or train your own CNN. If you're training your own, ensure you have a dataset with labeled pose information.

2. **Prepare Rotated Images**:
   - Choose a specific image for testing.
   - Create variations of this image by systematically rotating it through different angles (e.g., 0째, 45째, 90째, 135째, etc.).
   - Make sure rotations are applied without introducing additional artifacts or distortions.

3. **Feed the Rotated Images to the CNN**: For each rotated image, pass it through the CNN. This will give you feature maps at each layer of the network.

4. **Visualize Feature Maps**:
   - For each rotation, extract and visualize the feature maps at various layers. Depending on the depth of the CNN, you might choose initial, middle, and deeper layers.
   - Use heatmaps or grayscale representations to visualize the activations. Look for changes in the patterns as you compare the feature maps for each rotated image.

5. **Comparison and Analysis**:
   - Compare the feature maps for each orientation. Take note of which layers seem to be most sensitive to rotation changes and which layers are more invariant.
   - Look for patterns or specific features that activate consistently, regardless of the image's rotation.
   - Compare deeper layers to see if and how rotational information is preserved or abstracted as you progress through the network.

6. **Draw Conclusions**:
   - Determine which layers are most indicative of capturing pose-related information.
   - Understand how the spatial hierarchy of the CNN processes rotational changes. Often, earlier layers capture basic edges and textures, while deeper layers might capture more complex patterns and shapes that could be indicative of pose.

7. **Further Exploration** (optional):
   - To better understand pose calculation, consider incorporating specialized architectures or layers designed for rotation invariance, such as rotation-equivariant CNNs or spatial transformer networks.
   - Compare the behavior of standard CNNs with these specialized networks in terms of their feature map responses to rotated images.

Remember that understanding feature maps visually can be quite challenging, especially in deeper layers, due to the high-dimensionality and abstraction. However, with careful analysis and perhaps supplementary methods like dimensionality reduction or activation maximization, you can gain insights into how a CNN processes and understands image orientation.
"""

resnet_18 = models.resnet18(pretrained=True)
resnet_18.eval()

print(resnet_18)

folder_path = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"
count = 0
layers = [resnet_18.layer1]


# Let's Take the Image Tensors of a Group of Images
# Collected File names

file_paths = []

for filename in os.listdir(folder_path):
    if (filename.endswith(".jpg") or filename.endswith(".png")) and filename.startswith("shape_000"):
      count += 1
      file_path = os.path.join(folder_path, filename)
      file_paths.append(file_paths)

"""## We add Hooks for the Display of Feature Maps"""

import torch
import torchvision.models as models
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image

# Load the pre-trained ResNet-18 model
resnet_18 = models.resnet18(pretrained=True)
resnet_18.eval()

# Define the preprocessing transformations (assuming ImageNet statistics)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Collect feature maps
def collect_feature_maps(model, image, layers):
    feature_maps = {}
    hooks = []

    def hook_fn(module, input, output, key):
        feature_maps[key] = output

    for key, layer in layers.items():
        hooks.append(layer.register_forward_hook(lambda module, input, output, key=key: hook_fn(module, input, output, key)))

    with torch.no_grad():
        output = model(image)

    for hook in hooks:
        hook.remove()

    return feature_maps

# Visualize feature maps
def plot_feature_maps(key, feature_map):
    fmap = feature_map.cpu().squeeze().numpy()
    num_features = fmap.shape[0]
    grid_size = min(int(num_features**0.5), num_features)

    fig, axarr = plt.subplots(grid_size, grid_size, figsize=(20, 20))
    for idx in range(grid_size * grid_size):
        row = idx // grid_size
        col = idx % grid_size
        if idx < num_features:
          if fmap[idx].size != 1:  # ensures the tensor isn't scalar
            axarr[row, col].imshow(fmap[idx], cmap="inferno")
          else:
              print(f"Skipping scalar feature map at index {idx}.")
          axarr[row, col].axis('off')
        else:
            axarr[row, col].set_visible(False)
    plt.suptitle(f'Feature Maps from {key} layer', fontsize=16)
    plt.show()

# Define layers of interest
layers = {
    "beginning": resnet_18.conv1,
    "middle": resnet_18.layer2[-1],
    "end": list(resnet_18.children())[-2]
}

folder_path = "/content/drive/MyDrive/iscr_image_dataset/different_shapes"


for filename in os.listdir(folder_path):
    if (filename.endswith(".jpg") or filename.endswith(".png")) and filename.startswith("shape_000"):
      count += 1
      file_path = os.path.join(folder_path, filename)
      img = Image.open(file_path).convert('RGB')
      img_preprocessed = preprocess(img)
      img_tensor = torch.unsqueeze(img_preprocessed, 0)
      fmaps = collect_feature_maps(resnet_18, img_tensor, layers)
      for key in ["beginning", "middle", "end"]:
          plot_feature_maps(key, fmaps[key])

